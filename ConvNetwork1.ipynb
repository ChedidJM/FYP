{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./PyTorch/Data', train=True,download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "batch = next(iter(train_loader))\n",
    "images, labels=batch\n",
    "\n",
    "def Correct_Answers(predictions, labels):\n",
    "    return predictions.argmax(dim=1).eq(labels).sum().item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.ConvLayer1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.ConvLayer2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.LinLayer1 = nn.Linear(in_features=192, out_features=120)\n",
    "        self.LinLayer2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.OutLayer = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):  # t is the input tensor \n",
    "        t=self.ConvLayer1(t)\n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        t=self.ConvLayer2(t)\n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        t=t.reshape(-1, 192)\n",
    "        \n",
    "        t=self.LinLayer1(t)\n",
    "        t=F.relu(t)\n",
    "        \n",
    "        t=self.LinLayer2(t)\n",
    "        t=F.relu(t)\n",
    "        \n",
    "        t=self.OutLayer(t)\n",
    "        \n",
    "        return t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0251,  0.1406,  0.0597,  0.0996, -0.1386],\n",
       "          [ 0.1490, -0.1499,  0.0659,  0.1222, -0.0075],\n",
       "          [-0.1687,  0.1827, -0.1810,  0.1989,  0.0238],\n",
       "          [ 0.0201, -0.0927,  0.0028,  0.1798, -0.0237],\n",
       "          [ 0.1959,  0.1137,  0.1298,  0.0876, -0.0474]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1355, -0.1127,  0.0523,  0.0591, -0.0099],\n",
       "          [ 0.0478,  0.0718,  0.0678, -0.1342,  0.1506],\n",
       "          [-0.1575,  0.1162, -0.1510,  0.0239, -0.1218],\n",
       "          [-0.1932, -0.0193, -0.0095,  0.1717,  0.1719],\n",
       "          [ 0.0303, -0.0413,  0.1463, -0.1431, -0.0294]]],\n",
       "\n",
       "\n",
       "        [[[-0.1781, -0.1155, -0.1749, -0.0521,  0.1849],\n",
       "          [ 0.1198,  0.0063, -0.1662,  0.0661,  0.1249],\n",
       "          [-0.1945, -0.0989, -0.0921, -0.1310,  0.1325],\n",
       "          [ 0.1165,  0.1953,  0.0402, -0.1740, -0.0856],\n",
       "          [ 0.0739,  0.1949,  0.1406, -0.1704, -0.0493]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1326, -0.1689,  0.1723,  0.0503,  0.1233],\n",
       "          [-0.1435, -0.0692,  0.1486, -0.1537,  0.1753],\n",
       "          [-0.1310, -0.0621, -0.1060, -0.0082,  0.1386],\n",
       "          [-0.1643,  0.0101,  0.0797, -0.0720, -0.1101],\n",
       "          [ 0.1271,  0.0211,  0.0199,  0.0279, -0.1977]]],\n",
       "\n",
       "\n",
       "        [[[-0.0593,  0.1095,  0.1737, -0.0267,  0.1869],\n",
       "          [-0.1033,  0.0513,  0.0555, -0.0211,  0.0790],\n",
       "          [-0.1150,  0.0575, -0.0418, -0.1627,  0.0227],\n",
       "          [ 0.0985,  0.0786,  0.1196,  0.0255, -0.0529],\n",
       "          [ 0.0577,  0.1456,  0.1646,  0.0014, -0.0716]]],\n",
       "\n",
       "\n",
       "        [[[-0.1774, -0.1584,  0.0989,  0.0733,  0.0801],\n",
       "          [-0.0262, -0.1712, -0.0132, -0.1620, -0.0805],\n",
       "          [-0.1463, -0.1764,  0.0983, -0.0495,  0.1887],\n",
       "          [ 0.0723, -0.0878,  0.0909,  0.0502,  0.1160],\n",
       "          [ 0.0685,  0.0626, -0.1338, -0.0558, -0.0153]]]], requires_grad=True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyConvNet = ConvNet()\n",
    "MyConvNet.ConvLayer1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3210463523864746"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predectionsTensor = MyConvNet(images) # images are the training images chosen from the FashionMNIST dataset\n",
    "loss=F.cross_entropy(predectionsTensor, labels) #calculating the loss of our network\n",
    "loss.item()\n",
    "loss.backward() # Calculating Gradients with backward propagation to improve the weights\n",
    "\n",
    "optimizer=optim.Adam(MyConvNet.parameters(), lr=0.01) # creating the Adam optimizer (other optimizers like SGD avai;able)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correct_Answers(predectionsTensor, labels) # 4 correct answers out of 100 predictions in the first forward pass \n",
    "                                           # through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step() #optimizing the network weights\n",
    "predectionsTensor = MyConvNet(images)\n",
    "loss=F.cross_entropy(predectionsTensor, labels) #Calculating the loss after optimizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2836971282958984\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())\n",
    "print(Correct_Answers(predectionsTensor, labels)) # The number of correct predictions improved from 4 to 12\n",
    "                                                  # after one optimizaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
